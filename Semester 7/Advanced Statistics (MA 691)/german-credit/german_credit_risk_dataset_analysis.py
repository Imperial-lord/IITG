# -*- coding: utf-8 -*-
"""German Credit Risk Dataset Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J6auH5XnrtWSmduiI2x71WhT_0BoUdyq

# **German Credit Risk Dataset Analysis**

The goal is to predict if this loan credit would be a risk to the bank or not?

In simple terms, if the loan amount is given to the applicant, will they pay back or become a defaulter?

Since there are many applications which needs to be processed everyday, it will be helpful if there was a predictive model in place which can assist the executives to do their job by giving them a heads up about approval or rejection of a new loan application.

# Import Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import classifiercobra
import time
import os
import pickle
from sklearn.preprocessing import MinMaxScaler
from sklearn.naive_bayes import GaussianNB, MultinomialNB
from sklearn import svm
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import RandomForestClassifier
from dtreeplt import dtreeplt
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import cross_val_score
from sklearn import metrics
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
import warnings
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib
import matplotlib.pyplot as plt

# %matplotlib inline

# Reading the data into python
df = pd.read_csv("train.csv")

# df.head()

"""# Data description(Data Dictionary)

The business meaning of each column in the data 

* `GoodCredit`: Whether the issued loan was a good decision or bad
* `checkingstatus`: Status of existing checking account.
* `duration`: Duration of loan in months
* `history`: Credit history of the applicant
* `purpose`: Purpose for the loan
* `amount`: Credit amount
* `savings`: Savings account/bonds
* `employ`: Present employment since
* `installment`: Installment rate in percentage of disposable income
* `status`: Personal status and sex
* `others`: Other debtors / guarantors for the applicant
* `residence`: Present residence since
* `property`: Property type of applicant
* `age`: Age in years
* `otherplans`: Other installment plans
* `housing`: Housing
* `cards`: Number of existing credits at this bank
* `job`: Job
* `liable`: Number of people being liable to provide maintenance for
* `tele`: Is the Telephone registered or not
* `foreign`: Is the applicant a foreign worker

# Data Exploration - Aimed at understanding the overall data
"""

# Number of rows and columns
# df.shape

# Descriptive statistics of the data
# df.describe(include='all')

# Summarized information of data- Data types, Missing values based on number of non-null values Vs total rows etc.
# df.info()

# Number of Unique variable in each column
# df.nunique()

# Any Null-Value
# df.isnull().sum()

# Removing duplicate rows if any
df = df.drop_duplicates()

# checking shape of Data after removing Duplicates
# df.shape

"""# Basic Data Exploration Results:

Target Variable: **GoodCredit**

**Predictors**: *duration, history, purpose, amount, savings*, etc.

* GoodCredit = 1 means the loan was a good decision.
* GoodCredit = 0 means the loan was a bad decision.

**Determining the type of Machine Learning -**

Based on the problem statement I can understand that we need to create a supervised ML classification model, as the target variable is categorical.

# -Looking at the distribution of Target variable
"""

# Creating Bar chart as the Target variable is Categorical
# GroupedData = df.groupby('GoodCredit').size()
# GroupedData.plot(kind='bar', figsize=(4, 3))

"""The data distribution of the target variable is satisfactory to proceed further. There are sufficient number of rows for each category to learn from.

# Visual Exploratory Data Analysis

1. **Categorical variales**: `'checkingstatus', 'history', 'purpose','savings','employ', 'installment', 'status', 'others','residence', 'property', 'otherplans', 'housing', 'cards', 'job', 'liable', 'tele', 'foreign'`


2. **Continuous variables**: `'amount', 'age', 'duration'`.

# Plotting bar charts for categorical variable
"""

# plt.figsize = (20, 20)
# sns.countplot(x=df['checkingstatus'])
# plt.title('checkingstatus Distribution')
# plt.show()

# plt.figsize = (20, 20)
# sns.countplot(x=df['history'])
# plt.title('history Distribution')
# plt.show()

# plt.figsize = (20, 20)
# sns.countplot(x=df['purpose'])
# plt.title('purpose Distribution')
# plt.show()

# plt.figsize = (20, 20)
# sns.countplot(x=df['savings'])
# plt.title('savings Distribution')
# plt.show()

# plt.figsize = (20, 20)
# sns.countplot(x=df['employ'])
# plt.title('employ Distribution')
# plt.show()

# plt.figsize = (20, 20)
# sns.countplot(x=df['installment'])
# plt.title('installment Distribution')
# plt.show()

# plt.figsize = (20, 20)
# sns.countplot(x=df['status'])
# plt.title('status Distribution')
# plt.show()

# plt.figsize = (20, 20)
# sns.countplot(x=df['others'])
# plt.title('others Distribution')
# plt.show()

# plt.figsize = (20, 20)
# sns.countplot(x=df['residence'])
# plt.title('residence Distribution')
# plt.show()

# plt.figsize = (20, 20)
# sns.countplot(x=df['property'])
# plt.title('property Distribution')
# plt.show()

# plt.figsize = (20, 20)
# sns.countplot(x=df['otherplans'])
# plt.title('otherplans Distribution')
# plt.show()

# plt.figsize = (20, 20)
# sns.countplot(x=df['housing'])
# plt.title('housing Distribution')
# plt.show()

# plt.figsize = (20, 20)
# sns.countplot(x=df['cards'])
# plt.title('cards Distribution')
# plt.show()

# plt.figsize = (20, 20)
# sns.countplot(x=df['job'])
# plt.title('job Distribution')
# plt.show()

# plt.figsize = (20, 20)
# sns.countplot(x=df['liable'])
# plt.title('liable Distribution')
# plt.show()

# plt.figsize = (20, 20)
# sns.countplot(x=df['tele'])
# plt.title('tele Distribution')
# plt.show()

# plt.figsize = (20, 20)
# sns.countplot(x=df['foreign'])
# plt.title('foreign Distribution')
# plt.show()

"""# Bar Charts Interpretation
---

These bar charts represent the frequencies of each category in the Y-axis and the category names in the X-axis.

The ideal bar chart is where each category has comparable frequency. Hence, there are enough rows for each category in the data for the ML algorithm to learn.

If there is a column which shows too skewed distribution where there is only one dominant bar and the other categories are present in very low numbers. These kind of columns may not be very helpful in machine learning. We will confirm this in the correlation analysis section and take a final call to select or reject the column.

# Plotting histograms for continuous variables
"""

# df.hist(['age', 'amount', 'duration'], figsize=(18, 10), rwidth=0.95)

"""# Histogram Interpretation:
Histograms shows us the data distribution for a single continuous variable.

The X-axis shows the range of values and Y-axis represent the number of values in that range.

# Missing values treatment
"""

# df.isnull().sum()

"""We see that we have no `null` values so no treatment is needed

# Feature Selection:
---

Now its time to finally choose the best columns(Features) which are correlated to the Target variable. 

We will do this by visualizing the relation between the Target variable and each of the predictors to get a better sense of data. 

Then we will directly measure the correlation values or ANOVA or Chi-Square tests.

1. **Relationship exploration**: Box plots for Categorical Target Variable "GoodCredit" and continuous predictors
"""

# ContinuousColsList = ['age', 'amount', 'duration']

# fig, PlotCanvas = plt.subplots(
#     nrows=1, ncols=len(ContinuousColsList), figsize=(18, 5))

# # Creating box plots for each continuous predictor against the Target Variable "GoodCredit"
# for PredictorCol, i in zip(np.array(ContinuousColsList), range(len(ContinuousColsList))):
#     df.boxplot(column=PredictorCol, by='GoodCredit',
#                figsize=(5, 5), vert=True, ax=PlotCanvas[i])

"""# Box-Plots interpretation: 
These plots gives an idea about the data distribution of continuous predictor in the Y-axis for each of the category in the X-Axis.

If the distribution looks similar for each category(Boxes are in the same line), that means the the continuous variable has NO effect on the target variable. Hence, the variables are not correlated to each other.

# Statistical Feature Selection using ANOVA test
"""

# Defining a function to find the statistical relationship with all the categorical variables


# def FunctionAnova(inpData, TargetVariable, ContinuousPredictorList):
#     from scipy.stats import f_oneway

#     # Creating an empty list of final selected predictors
#     SelectedPredictors = []

#     print('ANOVA Test Results \n')
#     for predictor in ContinuousPredictorList:
#         CategoryGroupLists = inpData.groupby(
#             TargetVariable)[predictor].apply(list)
#         AnovaResults = f_oneway(*CategoryGroupLists)

#         if (AnovaResults[1] < 0.05):
#             print(predictor, 'is correlated with',
#                   TargetVariable, '| P-Value:', AnovaResults[1])
#             SelectedPredictors.append(predictor)
#         else:
#             print(predictor, 'is NOT correlated with',
#                   TargetVariable, '| P-Value:', AnovaResults[1])

#     return(SelectedPredictors)


# Calling the function to check which categorical variables are correlated with target
# ContinuousVariables = ['age', 'amount', 'duration']
# FunctionAnova(inpData=df, TargetVariable='GoodCredit',
#               ContinuousPredictorList=ContinuousVariables)

"""All three columns are correlated with GoodCredit but the P-Value of "age", it is just at the boundry of the threshold.

# Relationship exploration: 
Grouped Bar Charts for Categorical Target Variable "GoodCredit" and Categorical predictors
"""

# CategoricalColsList = ['checkingstatus', 'history', 'purpose', 'savings', 'employ',
#                        'installment', 'status', 'others', 'residence', 'property',
#                        'otherplans', 'housing', 'cards', 'job', 'liable', 'tele', 'foreign']

# CrossTabResult = pd.crosstab(
#     index=df['checkingstatus'], columns=df['GoodCredit'])
# CrossTabResult

# CrossTabResult1 = pd.crosstab(index=df['history'], columns=df['GoodCredit'])
# CrossTabResult1

# Creating Grouped bar plots for each categorical predictor against the Target Variable "GoodCredit"

# fig, PlotCanvas = plt.subplots(
#     nrows=len(CategoricalColsList), ncols=1, figsize=(10, 90))

# for CategoricalCol, i in zip(CategoricalColsList, range(len(CategoricalColsList))):
#     CrossTabResult = pd.crosstab(
#         index=df[CategoricalCol], columns=df['GoodCredit'])
#     CrossTabResult.plot.bar(color=['red', 'green'], ax=PlotCanvas[i])

"""# Grouped Bar charts Interpretation: 
These grouped bar charts show the frequency in the Y-Axis and the category in the X-Axis.

If the ratio of bars is similar across all categories, then the two columns are not correlated.

# Statistical Feature Selection using Chi-Square Test

Chi-Square test is conducted to check the correlation between two categorical variables
"""

# Writing a function to find the correlation of all categorical variables with the Target variable


# def FunctionChisq(inpData, TargetVariable, CategoricalVariablesList):
#     from scipy.stats import chi2_contingency

#     # Creating an empty list of final selected predictors
#     SelectedPredictors = []
#     print('Chi Square Test Results \n')

#     for predictor in CategoricalVariablesList:
#         CrossTabResult = pd.crosstab(
#             index=inpData[TargetVariable], columns=inpData[predictor])
#         ChiSqResult = chi2_contingency(CrossTabResult)

#         if (ChiSqResult[1] < 0.05):
#             print(predictor, 'is correlated with',
#                   TargetVariable, '| P-Value:', ChiSqResult[1])
#             SelectedPredictors.append(predictor)
#         else:
#             print(predictor, 'is NOT correlated with',
#                   TargetVariable, '| P-Value:', ChiSqResult[1])

#     return(SelectedPredictors)


# FunctionChisq(inpData=df,
#               TargetVariable='GoodCredit',
#               CategoricalVariablesList=CategoricalColsList)

"""Based on the results of Chi-Square test, below categorical columns are selected as predictors for Machine Learning

'checkingstatus', 'history', 'purpose', 'savings', 'employ', 'status', 'others', 'property', 'otherplans', 'housing', 'foreign'

# Selecting final predictors for Machine Learning
"""

# Based on all the above tests, selecting the final columns for machine learning

SelectedColumns = [
    "checkingstatus",
    "history",
    "purpose",
    "savings",
    "employ",
    "status",
    "others",
    "property",
    "otherplans",
    "housing",
    "foreign",
    "age",
    "amount",
    "duration",
]

# Selecting final columns
DataForML = df[SelectedColumns]
# DataForML.head()

# Saving this final data for reference during deployment
DataForML.to_pickle("DataForML.pkl")

# Supressing the warning messages
warnings.filterwarnings("ignore")

"""# Data Pre-processing for Machine Learning

##### 1. Converting Ordinal variables to numeric using business mapping
"""

# Treating the Ordinal variable first
DataForML["employ"].replace(
    {"A71": 1, "A72": 2, "A73": 3, "A74": 4, "A75": 5}, inplace=True
)

# Converting the binary nominal variable to numeric using 1/0 mapping
# Treating the binary nominal variable

DataForML["foreign"].replace({"A201": 1, "A202": 0}, inplace=True)

"""##### 2. Converting nominal variables to numeric using get_dummies()"""

# Treating all the nominal variables at once using dummy variables
DataForML_Numeric = pd.get_dummies(DataForML)

# Adding Target Variable to the data
DataForML_Numeric["GoodCredit"] = df["GoodCredit"]

# Looking at data after all the treatments
# DataForML_Numeric.head()

# Printing all the column names for our reference
# DataForML_Numeric.columns

# Separate Target Variable and Predictor Variables
TargetVariable = "GoodCredit"
Predictors = [
    "employ",
    "foreign",
    "age",
    "amount",
    "duration",
    "checkingstatus_A11",
    "checkingstatus_A12",
    "checkingstatus_A13",
    "checkingstatus_A14",
    "history_A30",
    "history_A31",
    "history_A32",
    "history_A33",
    "history_A34",
    "purpose_A40",
    "purpose_A41",
    "purpose_A410",
    "purpose_A42",
    "purpose_A43",
    "purpose_A44",
    "purpose_A45",
    "purpose_A46",
    "purpose_A48",
    "purpose_A49",
    "savings_A61",
    "savings_A62",
    "savings_A63",
    "savings_A64",
    "savings_A65",
    "status_A91",
    "status_A92",
    "status_A93",
    "status_A94",
    "others_A101",
    "others_A102",
    "others_A103",
    "property_A121",
    "property_A122",
    "property_A123",
    "property_A124",
    "otherplans_A141",
    "otherplans_A142",
    "otherplans_A143",
    "housing_A151",
    "housing_A152",
    "housing_A153",
]

X = DataForML_Numeric[Predictors].values
y = DataForML_Numeric[TargetVariable].values

# Splitting the data into training and testing set

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=428
)

"""# Normalization of data

"""


PredictorScaler = MinMaxScaler()

# Storing the fit object for later reference
PredictorScalerFit = PredictorScaler.fit(X)

# Generating the standardized values of X
X = PredictorScalerFit.transform(X)

# Split the data into training and testing set

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Sanity check for the sampled data
# print(X_train.shape)
# print(y_train.shape)
# print(X_test.shape)
# print(y_test.shape)

"""# **After all the treatments and splitting the final data into test and train we will now starting modelling diffrent classification models.**

# Classifier 1- Logistic Regression
"""

"""
clf = LogisticRegression(C=1, penalty='l2', solver='newton-cg')
clf

# Creating the model on Training Data
LOG = clf.fit(X_train, y_train)
prediction = LOG.predict(X_test)

# Measuring accuracy on Testing Data
print(metrics.classification_report(y_test, prediction))
print(metrics.confusion_matrix(y_test, prediction))

# the Overall Accuracy of the model
F1_Score = metrics.f1_score(y_test, prediction, average='weighted')
print('Accuracy of the model on Testing Sample Data:', round(F1_Score, 2))

# Importing cross validation function from sklearn

# Running Cross validation

Accuracy_Values = cross_val_score(LOG, X, y, cv=10, scoring='f1_weighted')
print('\nAccuracy values for 10-fold Cross Validation:\n', Accuracy_Values)
print('\nFinal Average Accuracy of the model:',
      round(Accuracy_Values.mean(), 2))

# Calculating roc_auc_score
roc_auc_score(y_test, prediction)

pred_proba = clf.predict_proba(X_test)


pred_proba = clf.predict_proba(X_test)[::, 1]

fpr, tpr, _ = roc_curve(y_test,  prediction)

auc = roc_auc_score(y_test, prediction)
plt.plot(fpr, tpr, label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.show()
"""

"""For more exploration - we can use oversampling, undersampling, smote and adasym

# Classifier 2 - Decision Tree Classifier
"""

"""
clf = tree.DecisionTreeClassifier(max_depth=4, criterion='gini')


# Creating the model on Training Data
DTree = clf.fit(X_train, y_train)
prediction = DTree.predict(X_test)

# Measuring accuracy on Testing Data
print(metrics.classification_report(y_test, prediction))
print(metrics.confusion_matrix(y_test, prediction))

# Printing the Overall Accuracy of the model
F1_Score = metrics.f1_score(y_test, prediction, average='weighted')
print('Accuracy of the model on Testing Sample Data:', round(F1_Score, 2))


# Running Cross validation
Accuracy_Values = cross_val_score(DTree, X, y, cv=10, scoring='f1_weighted')
print('\nAccuracy values for 10-fold Cross Validation:\n', Accuracy_Values)
print('\nFinal Average Accuracy of the model:',
      round(Accuracy_Values.mean(), 2))

# Commented out IPython magic to ensure Python compatibility.
# Plotting the feature importance for Top 10 most important columns
# %matplotlib inline
feature_importances = pd.Series(DTree.feature_importances_, index=Predictors)
feature_importances.nlargest(10).plot(kind='barh')

# Calculating roc_auc_score

roc_auc_score(y_test, prediction)


pred_proba = clf.predict_proba(X_test)[::, 1]

fpr, tpr, _ = roc_curve(y_test,  prediction)

auc = roc_auc_score(y_test, prediction)
plt.plot(fpr, tpr, label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.show()

dtree = dtreeplt(model=clf, feature_names=Predictors,
                 target_names=TargetVariable)
fig = dtree.view()
currentFigure = plt.gcf()
currentFigure.set_size_inches(80, 40)
"""

"""# Classifier 3- Random Forest"""

"""
clf = RandomForestClassifier(max_depth=10, n_estimators=100, criterion='gini')
clf

# Creating the model on Training Data
RF = clf.fit(X_train, y_train)
prediction = RF.predict(X_test)

# Measuring accuracy on Testing Data
print(metrics.classification_report(y_test, prediction))
print(metrics.confusion_matrix(y_test, prediction))

# Printing the Overall Accuracy of the model
F1_Score = metrics.f1_score(y_test, prediction, average='weighted')
print('Accuracy of the model on Testing Sample Data:', round(F1_Score, 2))

Accuracy_Values = cross_val_score(RF, X, y, cv=10, scoring='f1_weighted')
print('\nAccuracy values for 10-fold Cross Validation:\n', Accuracy_Values)
print('\nFinal Average Accuracy of the model:',
      round(Accuracy_Values.mean(), 2))

roc_auc_score(y_test, prediction)

pred_proba = RF.predict_proba(X_test)[::, 1]

fpr, tpr, _ = roc_curve(y_test,  prediction)

auc = roc_auc_score(y_test, prediction)
plt.plot(fpr, tpr, label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# Plotting the feature importance for Top 10 most important columns
# %matplotlib inline
feature_importances = pd.Series(RF.feature_importances_, index=Predictors)
feature_importances.nlargest(10).plot(kind='barh')
"""

"""# Classifier4- AdaBoost"""

"""
DTC = DecisionTreeClassifier(max_depth=4)
clf = AdaBoostClassifier(
    n_estimators=200, base_estimator=DTC, learning_rate=0.01)
clf

# Creating the model on Training Data
AB = clf.fit(X_train, y_train)
prediction = AB.predict(X_test)

# Measuring accuracy on Testing Data
print(metrics.classification_report(y_test, prediction))
print(metrics.confusion_matrix(y_test, prediction))

# Printing the Overall Accuracy of the model
F1_Score = metrics.f1_score(y_test, prediction, average='weighted')
print('Accuracy of the model on Testing Sample Data:', round(F1_Score, 2))

# Importing cross validation function from sklearn

Accuracy_Values = cross_val_score(AB, X, y, cv=10, scoring='f1_weighted')
print('\nAccuracy values for 10-fold Cross Validation:\n', Accuracy_Values)
print('\nFinal Average Accuracy of the model:',
      round(Accuracy_Values.mean(), 2))

roc_auc_score(y_test, prediction)

pred_proba = AB.predict_proba(X_test)

pred_proba = AB.predict_proba(X_test)[::, 1]

fpr, tpr, _ = roc_curve(y_test,  prediction)

auc = roc_auc_score(y_test, prediction)
plt.plot(fpr, tpr, label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.show()
"""

# Commented out IPython magic to ensure Python compatibility.
# Plotting the feature importance for Top 10 most important columns
# %matplotlib inline
# feature_importances = pd.Series(AB.feature_importances_, index=Predictors)
# feature_importances.nlargest(10).plot(kind='barh')


"""# Classfier5- Xtreme Gradient Boosting (XGBoost)"""
"""
clf = XGBClassifier(max_depth=10, learning_rate=0.01,
                    n_estimators=200, objective='binary:logistic', booster='gbtree')
clf

# Creating the model on Training Data
XGB = clf.fit(X_train, y_train)
prediction = XGB.predict(X_test)

# Measuring accuracy on Testing Data
print(metrics.classification_report(y_test, prediction))
print(metrics.confusion_matrix(y_test, prediction))

# Printing the Overall Accuracy of the model
F1_Score = metrics.f1_score(y_test, prediction, average='weighted')
print('Accuracy of the model on Testing Sample Data:', round(F1_Score, 2))

# Importing cross validation function from sklearn

# Running 10-Fold Cross validation
Accuracy_Values = cross_val_score(XGB, X, y, cv=10, scoring='f1_weighted')
print('\nAccuracy values for 10-fold Cross Validation:\n', Accuracy_Values)
print('\nFinal Average Accuracy of the model:',
      round(Accuracy_Values.mean(), 2))

roc_auc_score(y_test, prediction)

pred_proba = XGB.predict_proba(X_test)

pred_proba = XGB.predict_proba(X_test)[::, 1]

fpr, tpr, _ = roc_curve(y_test,  prediction)

auc = roc_auc_score(y_test, prediction)
plt.plot(fpr, tpr, label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# Plotting the feature importance for Top 10 most important columns
# %matplotlib inline
feature_importances = pd.Series(XGB.feature_importances_, index=Predictors)
feature_importances.nlargest(10).plot(kind='barh')
"""

"""# Classifier6- K-Nearest Neighbor(KNN)"""

"""
clf = KNeighborsClassifier(n_neighbors=3)
clf

# Creating the model on Training Data
KNN = clf.fit(X_train, y_train)
prediction = KNN.predict(X_test)

# Measuring accuracy on Testing Data
print(metrics.classification_report(y_test, prediction))
print(metrics.confusion_matrix(y_test, prediction))

# Printing the Overall Accuracy of the model
F1_Score = metrics.f1_score(y_test, prediction, average='weighted')
print('Accuracy of the model on Testing Sample Data:', round(F1_Score, 2))

# Importing cross validation function from sklearn

Accuracy_Values = cross_val_score(KNN, X, y, cv=10, scoring='f1_weighted')
print('\nAccuracy values for 10-fold Cross Validation:\n', Accuracy_Values)
print('\nFinal Average Accuracy of the model:',
      round(Accuracy_Values.mean(), 2))

roc_auc_score(y_test, prediction)

pred_proba = KNN.predict_proba(X_test)


pred_proba = KNN.predict_proba(X_test)[::, 1]

fpr, tpr, _ = roc_curve(y_test,  prediction)

auc = roc_auc_score(y_test, prediction)
plt.plot(fpr, tpr, label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.show()

"""  # Classifier7- Support Vector Machines(SVM)"""

"""
clf = svm.SVC(C=2, kernel='rbf', gamma=0.1)
clf

# Creating the model on Training Data
SVM = clf.fit(X_train, y_train)
prediction = SVM.predict(X_test)

# Measuring accuracy on Testing Data
print(metrics.classification_report(y_test, prediction))
print(metrics.confusion_matrix(y_test, prediction))

# Printing the Overall Accuracy of the model
F1_Score = metrics.f1_score(y_test, prediction, average='weighted')
print('Accuracy of the model on Testing Sample Data:', round(F1_Score, 2))

# Importing cross validation function from sklearn

Accuracy_Values = cross_val_score(SVM, X, y, cv=10, scoring='f1_weighted')
print('\nAccuracy values for 10-fold Cross Validation:\n', Accuracy_Values)
print('\nFinal Average Accuracy of the model:',
      round(Accuracy_Values.mean(), 2))

roc_auc_score(y_test, prediction)
"""

"""# Classifier8- Naive Bays"""

"""
clf = GaussianNB()
clf

NB = clf.fit(X_train, y_train)
prediction = NB.predict(X_test)

# Measuring accuracy on Testing Data
print(metrics.classification_report(y_test, prediction))
print(metrics.confusion_matrix(y_test, prediction))

# Printing the Overall Accuracy of the model
F1_Score = metrics.f1_score(y_test, prediction, average='weighted')
print('Accuracy of the model on Testing Sample Data:', round(F1_Score, 2))

# Importing cross validation function from sklearn
Accuracy_Values = cross_val_score(NB, X, y, cv=10, scoring='f1_weighted')
print('\nAccuracy values for 10-fold Cross Validation:\n', Accuracy_Values)
print('\nFinal Average Accuracy of the model:',
      round(Accuracy_Values.mean(), 2))

roc_auc_score(y_test, prediction)

pred_proba = NB.predict_proba(X_test)

pred_proba = NB.predict_proba(X_test)[::, 1]

fpr, tpr, _ = roc_curve(y_test,  prediction)

auc = roc_auc_score(y_test, prediction)
plt.plot(fpr, tpr, label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.show()
"""

"""# FINAL OUTPUT OF ALL THE MODELS

# 1.Logistic Regression
Accuracy of the model on Testing Sample Data: 0.76

Cross validation Average Accuracy of the model: 0.74

roc_auc_score: 0.69

# 2.Decision Tree Classifier
Accuracy of the model on Testing Sample Data: 0.67

Cross validation verage Accuracy of the model: 0.7

roc_auc_score: 0.59

# 3.RandomForestClassifier
Accuracy of the model on Testing Sample Data: 0.72

cross validation: 0.74

roc_auc_score: 0.63

# 4.AdaBoost
Accuracy of the model on Testing Sample Data: 0.73

Cross Validation Average Accuracy of the model: 0.74

roc_auc_score: 0.66

# 5.XGBoost
Accuracy of the model on Testing Sample Data: 0.72

Cross Validation Average Accuracy of the model: 0.75

roc_auc_score: 0.64

# 6.KNN
Accuracy of the model on Testing Sample Data: 0.74

Cross Validation Average Accuracy of the model: 0.7

roc_auc_score: 0.67

# 7.SVM
Accuracy of the model on Testing Sample Data: 0.73

Cross Validation Average Accuracy of the model: 0.75

roc_auc_score:0.65

# 8.Naive Bays
Accuracy of the model on Testing Sample Data: 0.72

Cross Validation Average Accuracy of the model: 0.7

roc_auc_score: 0.71

ALL THE MODELS HAVE NEARLY SAME ACCURACY ON TEST SAMPLE AS WELL AS AVERAGE CROSS VALIDATION IS ALSO SAME BUT IN MOST OF THE MODEL ROC_AUC_SCORE HAD A HUGE DIFFRENCE BETWEEN IT SELF AND ACCURACY OF MODEL

We can try more by Fixing the class Imbalance and apply some sampling techniques(OVERSAMPLING, UNDERSAMPLING,SMOTE,ADASYN) by this we MAY OR MAY NOT HAVE BETTER RESULTS

# Deployment of the Model
Based on the above trials we select that algorithm which produces the best average accuracy,Average cross validation accuracy, roc_auc_score and does not have much difference between all three . In this case, multiple algorithms have produced similar kind of average accuracy. Hence, we can choose any one of them.

I am choosing NB as the final model 

For deploying the model we will follow these steps:

Train the model using 100% data available

Save the model as a serialized file which can be stored anywhere

Create a python function which gets integrated with front-end(Tableau/Java Website etc.) to take all the inputs and returns the prediction

Choosing only the most important variables

Its beneficial to keep lesser number of predictors for the model while deploying it in production. The lesser predictors you keep, the better because, the model will be less dependent hence, more stable and it takes lesser time.

In this data, the most important predictor variables are 'employ', 'age', 'amount', 'duration','checkingstatus', 'history', 'purpose', 'savings', and 'status'. As these are consistently on top of the variable importance chart for every algorithm. Hence choosing these as final set of predictor variables.
"""

# Separate Target Variable and Predictor Variables
TargetVariable = "GoodCredit"

# Selecting the final set of predictors for the deployment
Predictors = [
    "employ",
    "age",
    "amount",
    "duration",
    "checkingstatus_A11",
    "checkingstatus_A12",
    "checkingstatus_A13",
    "checkingstatus_A14",
    "history_A30",
    "history_A31",
    "history_A32",
    "history_A33",
    "history_A34",
    "purpose_A40",
    "purpose_A41",
    "purpose_A410",
    "purpose_A42",
    "purpose_A43",
    "purpose_A44",
    "purpose_A45",
    "purpose_A46",
    "purpose_A48",
    "purpose_A49",
    "savings_A61",
    "savings_A62",
    "savings_A63",
    "savings_A64",
    "savings_A65",
    "status_A91",
    "status_A92",
    "status_A93",
    "status_A94",
]

X = DataForML_Numeric[Predictors].values
y = DataForML_Numeric[TargetVariable].values

### Normalization of data ###

PredictorScaler = MinMaxScaler()

PredictorScalerFit = PredictorScaler.fit(X)

X = PredictorScalerFit.transform(X)

# print(X.shape)
# print(y.shape)

# Retraining the model using 100% data
# Using the naive_bayes algorithm with final hyperparamters


# clf = GaussianNB()
# clf

clf = classifiercobra.ClassifierCobra(machine_list="advanced")

# Training the model on 100% Data available
Final_COBRA_Model = clf.fit(X, y)

# Cross validating the final model accuracy with less predictors
# Importing cross validation function from sklearn
Accuracy_Values = cross_val_score(Final_COBRA_Model, X, y, cv=10, scoring="f1_weighted")
print("\nAccuracy values for 10-fold Cross Validation:\n", Accuracy_Values)
print("\nFinal Average Accuracy of the model:", round(Accuracy_Values.mean(), 2))

# Save the model as a serialized file which can be stored anywhere

with open("Final_COBRA_Model.pkl", "wb") as fileWriteStream:
    pickle.dump(Final_COBRA_Model, fileWriteStream)

    fileWriteStream.close()

print("pickle file of Predictive Model is saved at Location:", os.getcwd())

# Create a python function
# This Function can be called from any from any front end tool/website
start = time.time()


def PredictLoanStatus(InputLoanDetails):
    import pandas as pd

    Num_Inputs = InputLoanDetails.shape[0]

    # Appending the new data with the Training data
    DataForML = pd.read_pickle("DataForML.pkl")
    InputLoanDetails = InputLoanDetails.append(DataForML)

    # Treating the Ordinal variable first
    InputLoanDetails["employ"].replace(
        {"A71": 1, "A72": 2, "A73": 3, "A74": 4, "A75": 5}, inplace=True
    )

    # Generating dummy variables for rest of the nominal variables
    InputLoanDetails = pd.get_dummies(InputLoanDetails)

    # Maintaining the same order of columns as it was during the model training
    Predictors = [
        "employ",
        "age",
        "amount",
        "duration",
        "checkingstatus_A11",
        "checkingstatus_A12",
        "checkingstatus_A13",
        "checkingstatus_A14",
        "history_A30",
        "history_A31",
        "history_A32",
        "history_A33",
        "history_A34",
        "purpose_A40",
        "purpose_A41",
        "purpose_A410",
        "purpose_A42",
        "purpose_A43",
        "purpose_A44",
        "purpose_A45",
        "purpose_A46",
        "purpose_A48",
        "purpose_A49",
        "savings_A61",
        "savings_A62",
        "savings_A63",
        "savings_A64",
        "savings_A65",
        "status_A91",
        "status_A92",
        "status_A93",
        "status_A94",
    ]

    # Generating the input values to the model
    X = InputLoanDetails[Predictors].values[0:Num_Inputs]

    # Generating the standardized values of X since it was done while model training also
    X = PredictorScalerFit.transform(X)

    # Loading the Function from pickle file
    import pickle

    with open("Final_COBRA_Model.pkl", "rb") as fileReadStream:
        COBRA_model = pickle.load(fileReadStream)

        fileReadStream.close()

    # Genrating Predictions
    Prediction = COBRA_model.predict(X)
    PredictedStatus = pd.DataFrame(Prediction, columns=["Predicted Status"])
    return PredictedStatus


end = time.time()
print(end - start, "seconds")

# CHECKIN OUR FUNCTION is working or not BY Calling the function for some loan applications manually
NewLoanApplications = pd.DataFrame(
    data=[
        ["A73", 22, 5951, 48, "A12", "A32", "A43", "A61", "A92"],
        ["A72", 40, 8951, 24, "A12", "A32", "A43", "A61", "A92"],
    ],
    columns=[
        "employ",
        "age",
        "amount",
        "duration",
        "checkingstatus",
        "history",
        "purpose",
        "savings",
        "status",
    ],
)

print(NewLoanApplications)

# Calling the Function for prediction
print(PredictLoanStatus(InputLoanDetails=NewLoanApplications))


"""# Deploying a predictive model as an API"""

# Creating the function which can take loan inputs and perform prediction

"""
def FunctionLoanPrediction(inp_employ, inp_age, inp_amount, inp_duration,
                           inp_checkingstatus, inp_history, inp_purpose,
                           inp_savings, inp_status):
    SampleInputData = pd.DataFrame(
        data=[[inp_employ, inp_age, inp_amount, inp_duration,
               inp_checkingstatus, inp_history, inp_purpose, inp_savings, inp_status]],
        columns=['employ', 'age', 'amount', 'duration', 'checkingstatus',
                 'history', 'purpose', 'savings', 'status'])

    # Calling the function defined above using the input parameters
    Predictions = PredictLoanStatus(InputLoanDetails=SampleInputData)

    # Returning the predicted loan status
    return(Predictions.to_json())


# Function call
FunctionLoanPrediction(inp_employ='A73',  inp_age=22,  inp_amount=5951, inp_duration=48,
                       inp_checkingstatus='A12', inp_history='A32', inp_purpose='A43', inp_savings='A61', inp_status='A92')

"""

"""
# Creating Flask API

from flask import Flask,request,jsonify
import pickle
import pandas as pd
import numpy

app = Flask(__name__)

@app.route('/get_loan_prediction', methods=["GET"])
def get_loan_prediction():
    try:
        # Getting the paramters from API call
        employ_value = request.args.get('employ')
        age_value = float(request.args.get('age'))
        amount_value=float(request.args.get('amount'))
        duration_value=float(request.args.get('duration'))
        checkingstatus_value=request.args.get('checkingstatus')
        history_value=request.args.get('history')
        purpose_value=request.args.get('purpose')
        savings_value=request.args.get('savings')
        status_value=request.args.get('PropertyArea')
                
        # Calling the funtion to get loan approval status
        prediction_from_api=FunctionLoanPrediction(
                       inp_employ=employ_value,  inp_age= age_value,   inp_amount=amount_value,   inp_duration=duration_value,   inp_checkingstatus=checkingstatus_value,
                       inp_history=history_value,    inp_purpose=purpose_value,     inp_savings=savings_value,    inp_status=status_value)

        return (prediction_from_api)
    
    except Exception as e:
        return('Something is not right!:'+str(e))

import os
if __name__ =="__main__":
    
    # Hosting the API in localhost
    app.run(host='127.0.0.1', port=1010, threaded=True, debug=True, use_reloader=False)
    # Interrupt kernel to stop the API
"""
